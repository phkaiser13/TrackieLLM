<!--
This documentation was written by Jules - Google labs bot.
Original code by phkaiser13.
-->

# Audio Module

## 1. Overview

The Audio module serves as the "ears" and "voice" of the TrackieLLM system. It is responsible for all audio-related processing, enabling natural and intuitive interaction between the user and the AI. This includes listening for commands, transcribing speech, and providing spoken feedback.

Its primary function is to bridge the gap between human language and the system's internal logic, allowing users to control the device and receive information through conversation.

## 2. Core Responsibilities

-   **Wake-Word Detection:** Continuously listens for a specific "wake word" (e.g., "Hey Trackie") to activate the system's full listening mode, conserving power and ensuring privacy.
-   **Voice Activity Detection (VAD):** Detects the presence of human speech in the audio stream, distinguishing it from background noise. This is used to identify when the user is speaking.
-   **Automatic Speech Recognition (ASR):** Transcribes the user's spoken commands and questions into text that the Cortex module can understand and process.
-   **Text-to-Speech (TTS):** Synthesizes natural-sounding speech from text provided by the Cortex module, delivering alerts, descriptions, and answers to the user.
-   **Sound Analysis:** (Future implementation) Will identify and classify ambient sounds (e.g., alarms, sirens, running water) to provide additional environmental context to the Cortex.

## 3. Architecture and Models

The Audio module is a multi-stage pipeline designed for low-latency, on-device processing. It uses a combination of highly efficient models and C/Rust components to manage the audio stream.

### 3.1. AI Models

-   **Wake-Word Engine:**
    -   **Model:** `Porcupine`
    -   **Details:** A highly accurate and lightweight wake-word detection engine. It runs continuously in a low-power state.

-   **Voice Activity Detection (VAD):**
    -   **Model:** `Silero VAD`
    -   **Details:** A compact and reliable model for detecting speech. It is activated after the wake word is detected to determine the start and end of the user's utterance.

-   **Automatic Speech Recognition (ASR):**
    -   **Model:** `whisper.cpp tiny.en`
    -   **Format:** GGML
    -   **Details:** An efficient implementation of OpenAI's Whisper model, optimized for fast and accurate transcription on edge devices.

-   **Text-to-Speech (TTS):**
    -   **Model:** `Piper` (from Rhasspy)
    -   **Details:** A fast, local neural text-to-speech system that generates high-quality voices. The system is configured with pre-trained voices for different languages.

### 3.2. Pipeline Stages

The audio processing pipeline operates as follows:

1.  **Audio Capture:** A continuous stream of audio is captured from the device's microphone array.
2.  **Wake-Word Detection (`Porcupine`):** The stream is fed into the Porcupine engine. If the wake word is detected, the pipeline proceeds to the next stage.
3.  **VAD and Buffering (`Silero VAD`):** The Silero VAD model identifies the segments of the audio stream that contain speech. These segments are buffered for transcription.
4.  **Speech-to-Text (`whisper.cpp`):** The buffered audio segment is passed to the `whisper.cpp` engine, which transcribes it into text.
5.  **Event Dispatch:** The transcribed text is sent to the Cortex module as an `audio_command_t` event.
6.  **Receiving TTS Requests:** The Audio module listens for `tts_request_t` events from the Cortex, which contain text to be synthesized.
7.  **Speech Synthesis (`Piper`):** The text is sent to the Piper TTS engine, which generates an audio waveform.
8.  **Audio Playback:** The synthesized audio is played back to the user through the device's speakers or bone-conduction transducers.

## 4. Key Components

-   **`tk_audio_pipeline.c`:** Manages the overall audio pipeline, coordinating the different stages from capture to playback.
-   **`tk_vad_silero.c`:** An interface to the Silero VAD model.
-   **`tk_asr_whisper.c`:** A wrapper for the `whisper.cpp` library, handling the ASR process.
-   **`tk_tts_piper.c`:** A wrapper for the Piper TTS library, responsible for speech synthesis.
-   **`asr_processing.rs` / `tts_synthesis.rs`:** Rust components that provide safe and efficient data handling for the transcription and synthesis tasks.

## 5. Integration with other Modules

-   **Cortex:** The Audio module sends transcribed text to the Cortex and receives text to be synthesized. It is the primary interface for user interaction.
-   **Sensors:** Audio data can be correlated with sensor data (e.g., IMU) to suppress noise generated by user movement.
-   **Interaction:** The TTS output is a key part of the feedback provided to the user via the interaction manager.

## Developer and Author

*   **Original Code:** phkaiser13
*   **Documentation:** Jules - Google labs bot
